{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to CustomOp\n",
    "\n",
    "\n",
    "Need to create subclasses of `CustomOp` to provide execution, code generation and other functionality in FINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " 'execute_node',\n",
       " 'get_nodeattr',\n",
       " 'get_nodeattr_types',\n",
       " 'infer_node_datatype',\n",
       " 'make_shape_compatible_op',\n",
       " 'set_nodeattr',\n",
       " 'verify_node']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.custom_op import CustomOp\n",
    "dir(CustomOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points of importance:\n",
    "\n",
    "1. `CustomOp` instances (in Python) are not meant to store any data, only provide functionality on top of data stored in ONNX. Each `CustomOp` instance has a member `self.onnx_node` which gives access to the ONNX `NodeProto` with attributes. There is also a custom attribute setter/getter system in `CustomOp` to make this process easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `CustomOp` subclasses need to implement the methods above (those not starting with underscore) -- see custom_op/__init__.py for full details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `CustomOp` subclasses must be registered with the custom operator registry in `custom_op/registry.py` to be usable with the FINN framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple CustomOp Example\n",
    "\n",
    "Let's make a simple CustomOp that raises its input to a given exponent (specified as attribute). For now it'll only work in Python, but later we'll add C++ execution capability too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import helper\n",
    "import numpy as np\n",
    "\n",
    "class MyPythonPowerOp(CustomOp):\n",
    "    \n",
    "    # here we use the CustomOp attribute system to make it easier\n",
    "    # to set/get custom attributes on this node\n",
    "    def get_nodeattr_types(self):\n",
    "        return {\n",
    "            # each entry is:\n",
    "            # name of attribute : (dtype, required, default value)\n",
    "            # dtype follows the ONNX attribute protobuf so\n",
    "            # \"i\" is int, \"s\" is string, \"f\" is float,\n",
    "            # \"ints\" is a list of integers...\n",
    "            # also good practice to document what each attribute does here:\n",
    "            \n",
    "            # which integer power to raise the input to\n",
    "            \"exponent\" : (\"i\", True, 0),\n",
    "            # execution mode : currently only python\n",
    "            \"exec_mode\" : (\"s\", True, \"python\"),\n",
    "        }\n",
    "    \n",
    "    # return an ONNX node that has the same shape inference behavior\n",
    "    # here we want in shape = out shape, so we use the ONNX ReLU\n",
    "    # node to mimic its shape inference behavior\n",
    "    # we have access to the entire ModelWrapper to help make this decision\n",
    "    # (the parameter called model)\n",
    "    def make_shape_compatible_op(self, model):\n",
    "        node = self.onnx_node\n",
    "        # make a Relu node connected to the same in-out tensors to get\n",
    "        # shape inference\n",
    "        # a general-purpose alternative is to use a Constant node that \n",
    "        # produces the desired shape\n",
    "        return helper.make_node(\"Relu\", [node.input[0]], [node.output[0]])\n",
    "\n",
    "    # used for FINN DataType inference: set the output tensors' datatypes\n",
    "    # accordingly for this node\n",
    "    # here we assume input datatype = output datatype\n",
    "    # we have access to the entire ModelWrapper to help make this decision\n",
    "    # (the parameter called model)\n",
    "    def infer_node_datatype(self, model):\n",
    "        node = self.onnx_node\n",
    "        # data type stays the same\n",
    "        dtype = model.get_tensor_datatype(node.input[0])\n",
    "        model.set_tensor_datatype(node.output[0], dtype)\n",
    "    \n",
    "    # execute this node\n",
    "    # context: used for both input and output, dictionary of named\n",
    "    #          tensors\n",
    "    # graph: the ONNX GraphProto (ModelWrapper.graph), generally \n",
    "    #         not needed to execute a single node\n",
    "    def execute_node(self, context, graph):\n",
    "        exec_mode = self.get_nodeattr(\"exec_mode\")\n",
    "        if exec_mode == \"python\":\n",
    "            # get names of node input and output tensors\n",
    "            i_name = self.onnx_node.input[0]\n",
    "            o_name = self.onnx_node.output[0]\n",
    "            # grab input tensor from context\n",
    "            i_tensor = context[i_name]\n",
    "            # get which power to raise to from attribute\n",
    "            expnt = self.get_nodeattr(\"exponent\")\n",
    "            # compute and put output into context\n",
    "            o_tensor = np.power(i_tensor, expnt)\n",
    "            context[o_name] = o_tensor\n",
    "        else:\n",
    "            raise Exception(\"Only python exec_mode is supported\")\n",
    "        \n",
    "    # can use to do a sanity check of all the node's properties\n",
    "    # optional, not implemented here\n",
    "    def verify_node(self):\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to make it this usable in FINN we also have to register in in the custom op registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.custom_op.registry import custom_op\n",
    "\n",
    "custom_op[\"MyPythonPowerOp\"] = MyPythonPowerOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Try Out our CustomOp\n",
    "\n",
    "We'll manually build a small ONNX graph containing our node in order to try out some of the functionality. This would normally go into the unit test for this CustomOp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from onnx import TensorProto\n",
    "\n",
    "def make_graph(ishape, exp):\n",
    "    inp = helper.make_tensor_value_info(\n",
    "        \"inp\", TensorProto.FLOAT, ishape\n",
    "    )\n",
    "    outp = helper.make_tensor_value_info(\n",
    "        \"outp\", TensorProto.FLOAT, ishape\n",
    "    )\n",
    "\n",
    "    custom_node = helper.make_node(\n",
    "        # op type string in ONNX, what we used to register the custom op\n",
    "        \"MyPythonPowerOp\",\n",
    "        # name of input tensor\n",
    "        [\"inp\"],\n",
    "        # name of output tensor\n",
    "        [\"outp\"],\n",
    "        # needed for custom ops\n",
    "        domain=\"finn\",\n",
    "        # set up attributes\n",
    "        exponent = int(exp),\n",
    "        exec_mode = \"python\"\n",
    "    )\n",
    "\n",
    "    graph = helper.make_graph(\n",
    "        nodes=[custom_node], name=\"custom_graph\", inputs=[inp], outputs=[outp]\n",
    "    )\n",
    "    model = helper.make_model(graph, producer_name=\"custom-model\")\n",
    "    return ModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[input: \"inp\"\n",
       "output: \"outp\"\n",
       "op_type: \"MyPythonPowerOp\"\n",
       "attribute {\n",
       "  name: \"exec_mode\"\n",
       "  s: \"python\"\n",
       "  type: STRING\n",
       "}\n",
       "attribute {\n",
       "  name: \"exponent\"\n",
       "  i: 2\n",
       "  type: INT\n",
       "}\n",
       "domain: \"finn\"\n",
       "]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a small graph with our custom op\n",
    "input_shape = (1, 2, 4)\n",
    "ret_model = make_graph(input_shape, 2)\n",
    "ret_model.model.graph.node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-5.,  1., -2.,  7.],\n",
       "        [ 3., -1.,  4.,  5.]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.core.datatype import DataType\n",
    "from finn.util.basic import gen_finn_dt_tensor\n",
    "\n",
    "# generate a random input of e.g signed 4-bit values\n",
    "random_input = gen_finn_dt_tensor(DataType.INT4, input_shape)\n",
    "random_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outp': array([[[25.,  1.,  4., 49.],\n",
       "         [ 9.,  1., 16., 25.]]], dtype=float32)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "# run with FINN's execute_onnx\n",
    "inp_dict = {\"inp\" : random_input}\n",
    "ret = execute_onnx(ret_model, inp_dict)\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CustomOp with C++ Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir, CppBuilder\n",
    "\n",
    "# derive from our previous example\n",
    "class MyMixedPowerOp(MyPythonPowerOp):\n",
    "    \n",
    "    # here we use the CustomOp attribute system to make it easier\n",
    "    # to set/get custom attributes on this node\n",
    "    def get_nodeattr_types(self):\n",
    "        return {\n",
    "            # each entry is:\n",
    "            # name of attribute : (dtype, required, default value)\n",
    "            # dtype follows the ONNX attribute protobuf so\n",
    "            # \"i\" is int, \"s\" is string, \"f\" is float,\n",
    "            # \"ints\" is a list of integers...\n",
    "            # also good practice to document what each attribute does here:\n",
    "            \n",
    "            # which integer power to raise the input to\n",
    "            \"exponent\" : (\"i\", True, 0),\n",
    "            # execution mode : python or c++\n",
    "            \"exec_mode\" : (\"s\", True, \"python\"),\n",
    "            # code generation directory\n",
    "            \"codegen_dir\" : (\"s\", False, \"\"),\n",
    "        }\n",
    "    \n",
    "    def my_custom_cpp_gen(self):\n",
    "        build_dir = make_build_dir(prefix=\"my_custom_op\")\n",
    "        # set attribute for codegen dir\n",
    "        self.set_nodeattr(\"codegen_dir\", build_dir)\n",
    "        # generate some C++ code\n",
    "        cpp_code = \"\"\"\n",
    "#include <iostream>\n",
    "#include <fstream>\n",
    "using namespace std;\n",
    "#define EXPONENT %d\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    ifstream infile(\"input.txt\");\n",
    "    ofstream outfile(\"output.txt\");\n",
    "    \n",
    "    float elem;\n",
    "    while (infile >> elem)\n",
    "    {\n",
    "        float res = 1.0;\n",
    "        for(int i=0; i < EXPONENT; i++) {\n",
    "            res *= elem;\n",
    "        }\n",
    "        outfile << res << \"\\n\";\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "        \"\"\" % (self.get_nodeattr(\"exponent\"))\n",
    "        with open(build_dir+\"/top.cpp\", \"w\") as f:\n",
    "            f.write(cpp_code)\n",
    "        builder = CppBuilder()\n",
    "        # to enable additional debug features please uncommand the next line\n",
    "        builder.append_includes(\"--std=c++11\")\n",
    "        builder.append_includes(\"-O3\")\n",
    "        builder.append_sources(build_dir + \"/*.cpp\")\n",
    "        builder.set_executable_path(build_dir + \"/node_model\")\n",
    "        builder.build(code_gen_dir)\n",
    "    \n",
    "    # execute this node\n",
    "    # context: used for both input and output, dictionary of named\n",
    "    #          tensors\n",
    "    # graph: the ONNX GraphProto (ModelWrapper.graph), generally \n",
    "    #         not needed to execute a single node\n",
    "    def execute_node(self, context, graph):\n",
    "        exec_mode = self.get_nodeattr(\"exec_mode\")\n",
    "        # get names of node input and output tensors\n",
    "        i_name = self.onnx_node.input[0]\n",
    "        o_name = self.onnx_node.output[0]\n",
    "        # grab input tensor from context\n",
    "        i_tensor = context[i_name]\n",
    "        # get which power to raise to from attribute\n",
    "        expnt = self.get_nodeattr(\"exponent\")\n",
    "        if exec_mode == \"python\":\n",
    "            # compute and put output into context\n",
    "            o_tensor = np.power(i_tensor, expnt)\n",
    "            context[o_name] = o_tensor\n",
    "        elif exec_mode == \"c++\":\n",
    "            build_dir = self.get_nodeattr(\"codegen_dir\")\n",
    "            # save input as txt, could preprocess, change layout etc..\n",
    "            np.savetxt(build_dir+\"/input.txt\")\n",
    "            bash_command = [\"bash\", build_dir+\"/node_model\"]\n",
    "            proc_run = subprocess.Popen(bash_command, stdout=subprocess.PIPE)\n",
    "            proc_run.communicate()\n",
    "            np.loadtxt(build_dir+\"/output.txt\")\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Only python exec_mode is supported\")\n",
    "        \n",
    "    # can use to do a sanity check of all the node's properties\n",
    "    # optional, not implemented here\n",
    "    def verify_node(self):\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a code generation transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation import Transformation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
